### Example config for threshold estimation from single observations.
# Assuming you are doing threshold estimation and the intensity dimension
# is the final dimension, the only things that need to be changed for a
# typical experiment are:
# 1. parnames, lb and ub under [common], and optionally target.
# 2. n_trials under init_strat
# 3. n_trials under opt_strat
# 4. monotonic_idx under [MonotonicRejectionGP] if we you want to change it

## The common section includes global server parameters and parameters
## reused in multiple other classes
[common]
parnames = [hue, saturation, alpha] # names of the parameters
lb = [0, 0, 0] # lower bounds of the parameters, in the same order as above
ub = [1, 1, 0.8] # upper bounds of parameter, in the same order as above
outcome_type = single_probit # currently this is always single_probit (standard "1AFC" in psychophysics)
target = 0.75 # desired threshold, for threshold estimation. Otherwise is ignored or can be removed.

strategy_names = [init_strat, opt_strat] # The strategies that will be used, corresponding to the named sections below

# Configuration for the initialization strategy, which we use to gather initial points
# before we start doing model-based acquisition
[init_strat]
n_trials = 10 # number of sobol trials to run
generator = SobolGenerator # The generator class used to generate new parameter values

# Configuration for the optimization strategy, our model-based acquisition
[opt_strat]
# number of model-based trials to run
n_trials = 25
# The generator class used to generate new parameter values
generator = MonotonicRejectionGenerator

# On every trial, AEPsych can either initialize model hyperparameters randomly and refit
# from scratch, or initialize them to the previous model's parameters. This governs
# how often the former happens vs the latter. Using the previous model's parameters is often faster
# (leading to less waiting between trials) but occasional refitting is useful for avoiding local minima
# in hyperparameters, especially early in the experiment.
refit_every = 5


####################################################################################
# Advanced settings
####################################################################################
## The experiment section covers the main components of the experiment:
## the strategies used for initialization and optimization, the
## model used for the modeling part, and the acquisition
## function
[experiment]
# Acquisition function: the objective we use to decide where to sample next. We recommend
# MonotonicMCLSE for threshold finding, MonotonicMCPosteriorVariance for global exploration,
# qNoisyExpectedImprovement for optimization. For other options, check out the botorch and
# aepsych docs
acqf = MonotonicMCLSE
# The model. MonotonicRejectionGP or MonotonicRejectionGPLSETS
# are the options for monotonic, otherwise GPClassificationModel.
model = MonotonicRejectionGP


## Below this section are configurations of all the classes defined in the section above,
## matching the API in the code.


## Below this section are configurations of all the classes defined in the section above,
## matching the API in the code.

## MonotonicRejectionGP model settings.
[MonotonicRejectionGP]
# Number of inducing points for approximate inference. 100 is fine for 2d and overkill for 1d;
# for larger dimensions, scale this up.
inducing_size = 400
# A "factory" function (defined in aepsych) that builds the GP mean and covariance.
# other options are default_mean_covar_factory (for non-monotonic RBF kernel, for use with
# GPClassificationModel) and song_mean_covar_factory (for the linear-additive model of some
# previous work).
mean_covar_factory = monotonic_mean_covar_factory
# The set of monotonic indices, in 0-based indexing. The model works fine for any indices
# here but some of the AEPsych plotting code assumes there is only one monotonic dimension
# and it's the last dimension.
monotonic_idxs = [2]

## MonotonicRejectionGenerator model settings.
[MonotonicRejectionGenerator]
# number of restarts for acquisition function optimization.
restarts = 10
# number of samples for quasi-random initialization of the acquisition function optimizer
samps = 1000

## Acquisition function settings; we recommend not changing this.
[MonotonicMCLSE]
# A sort of "temperature" parameter that affects how widely around the predicted
# threshold we try to sample, analogous to beta in the UCB family of algorithms.
# The 3.98 default corresponds to the "straddle" heuristic in Bryan et al. 2005.
beta = 3.98
# The transformation of the latent function before threshold estimation. ProbitObjective
# lets us express the target threshold in probability.
objective = ProbitObjective
